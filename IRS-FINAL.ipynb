{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MHDAH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MHDAH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import requests as rq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class crawler to retrive documents from web pages\n",
    "class crawler:\n",
    "\n",
    "    def __init__(self): \n",
    "        self.to_visit = list()\n",
    "        self.visited = set()\n",
    "\n",
    "    def fetch(self, url):#to get all the content of url (return content)\n",
    "        print('now fetching.. ', url)\n",
    "        res = requests.get(url)\n",
    "\n",
    "        return res.content\n",
    "\n",
    "    def get_current_url(self):#to get all url visited (reurn url visited and print it)\n",
    "        res = self.to_visit.pop(0)\n",
    "\n",
    "        while res in self.visited:\n",
    "            print('already visited', res)\n",
    "            res = self.to_visit.pop(0)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def get_links(self, current_url, content):#to get all links with http from html page\n",
    "        urls = re.findall('<a href=\"([^\"]+)\">', str(content)) \n",
    "        print('urls are', urls)\n",
    "        for url in urls:\n",
    "            if url[0] == '/':\n",
    "                url = current_url + url[1:]\n",
    "            pattern = re.compile('https?')\n",
    "            if pattern.match(url):\n",
    "                self.to_visit.append(url) # عم ضيفها عل  to visit\n",
    "\n",
    "    def crawl(self, url, depth=20):\n",
    "        self.to_visit.append(url)\n",
    "        while len(self.visited) <= depth:\n",
    "            for id in range(0, len(self.visited)):\n",
    "                current_url = self.get_current_url()                                                # get url from web page\n",
    "                content = self.fetch(current_url)                                                   # retrive content from web page\n",
    "                p_content = re.findall('<p>(.*?)</p>', str(content))                                # find paragraph of page\n",
    "                data = pd.DataFrame({'Doc_ID': id, 'URL': current_url, 'Content': p_content})       # make it as dataFrame\n",
    "                dataset = data.to_csv('dataset.csv', index=False)                                   # save final dataset file\n",
    "                self.visited.add(current_url)\n",
    "                self.get_links(current_url,content)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------Testing URLs-----------------------------------\n",
    "c = crawler()\n",
    "c.crawl('https://noobnotes.net/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs_and_clean():\n",
    "  get_url = rq.get(\"https://noobnotes.net/part-of-your-world-little-mermaid-disney/?solfege=false\")\n",
    "  get_text = get_url.text\n",
    "\n",
    "  soup = BeautifulSoup(get_text, \"html.parser\")\n",
    "\n",
    "  song_title = [i.text for i in soup.findAll('h1', {\n",
    "    \"class\": \"entry-title\"\n",
    "  })]\n",
    "\n",
    "\n",
    "  div = soup.find('div', {'class': 'post-content'})\n",
    "  par = div.findChildren('p', recursive=False)\n",
    "\n",
    "  # Retrieve Paragraphs\n",
    "  documents = []\n",
    "  documents_clean = []\n",
    "  for p in par:\n",
    "       # print(p)\n",
    "        documents = str(p)\n",
    "        line_words = documents.partition('<br')[2]\n",
    "        line_words = line_words.replace('</br>', '')\n",
    "        line_words = line_words.replace('</p>', '')\n",
    "        line_words = line_words.replace('/>', '')\n",
    "        line_words = line_words.replace('>', '')\n",
    "        line_words = line_words.replace('.', '')\n",
    "        line_words = line_words.replace(',', '')\n",
    "        line_words = line_words.replace('\"', '')\n",
    "        line_words = line_words.replace('?', '')\n",
    "        line_words = line_words.replace('!', '')\n",
    "        line_words = line_words.replace('~', '')\n",
    "        line_words = line_words.replace('(<', '')\n",
    "        line_words = line_words.replace(')', '')\n",
    "        line_words = line_words.replace('? ', '')\n",
    "        line_words = line_words.replace('</em', '')\n",
    "        line_words = line_words.replace('-', '')\n",
    "        documents_clean.append(line_words)\n",
    "        \n",
    "  return documents_clean\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', \"\\nLook at this stuff isn't it neat\", \"\\nWouldn't you think my collection's complete\", \"\\nWouldn't you think I'm the girl\", '\\nThe girl who has everything', '', '\\nLook at this trove treasures untold', '\\nHow many wonders can one cavern hold', '\\nLooking around here you think', \"\\nSure she's got everything\", '', \"\\nI've got gadgets and gizmos aplenty\", \"\\nI've got whozits and whatzits galore\", '\\nWant a thingamabob', \"\\nI've got twenty\", '\\nBut who cares No big deal', '\\nI want more', '', '\\nI wanna be where the people are', \"\\nI wanna see wanna see them dancing'\", '\\nWalking around on those', \"\\nWhat do you call 'em\", '\\nOh  feet', '', \"\\nFlippin' your fins you don't get too far\", '\\nLegs are required for jumping dancing', '\\nStrolling along down a', \"\\nWhat's that word again\", '\\nStreet', '', '\\nUp where they walk up where they run', '\\nUp where they stay all day in the sun', \"\\nWanderin' free  wish I could be\", '\\nPart of that world', '', '\\nWhat would I give', '\\nIf I could live out of these waters', '\\nWhat would I pay', '\\nTo spend a day warm on the sand', '', \"\\nBet'cha on land they understand\", \"\\nBet they don't reprimand their daughters\", '\\nBright young women sick of swimming', '\\nReady to stand', '', \"\\nI'm ready to know what the people know\", \"\\nAsk 'em my questions and get some answers\", \"\\nWhat's a fire and why does it\", \"\\nWhat's the word Burn\", '', \"\\nWhen's it my turn Wouldn't I love\", '\\nLove to explore that shore up above', '\\nOut of the sea wish I could be', '\\nPart of that world']\n"
     ]
    }
   ],
   "source": [
    "docs = retrieve_docs_and_clean()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "cleanedData = []\n",
    "for sentence in docs:\n",
    "  filtered_sentence = []\n",
    "  words = word_tokenize(sentence)\n",
    "      \n",
    "  for word in words:\n",
    "      if word not in stopWords:\n",
    "          filtered_sentence.append(word)\n",
    "  cleanedData.append(' '.join(filtered_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"Look stuff n't neat\",\n",
       " \"Would n't think collection 's complete\",\n",
       " \"Would n't think I 'm girl\",\n",
       " 'The girl everything',\n",
       " '',\n",
       " 'Look trove treasures untold',\n",
       " 'How many wonders one cavern hold',\n",
       " 'Looking around think',\n",
       " \"Sure 's got everything\",\n",
       " '',\n",
       " \"I 've got gadgets gizmos aplenty\",\n",
       " \"I 've got whozits whatzits galore\",\n",
       " 'Want thingamabob',\n",
       " \"I 've got twenty\",\n",
       " 'But cares No big deal',\n",
       " 'I want',\n",
       " '',\n",
       " 'I wan na people',\n",
       " \"I wan na see wan na see dancing '\",\n",
       " 'Walking around',\n",
       " \"What call 'em\",\n",
       " 'Oh feet',\n",
       " '',\n",
       " \"Flippin ' fins n't get far\",\n",
       " 'Legs required jumping dancing',\n",
       " 'Strolling along',\n",
       " \"What 's word\",\n",
       " 'Street',\n",
       " '',\n",
       " 'Up walk run',\n",
       " 'Up stay day sun',\n",
       " \"Wanderin ' free wish I could\",\n",
       " 'Part world',\n",
       " '',\n",
       " 'What would I give',\n",
       " 'If I could live waters',\n",
       " 'What would I pay',\n",
       " 'To spend day warm sand',\n",
       " '',\n",
       " \"Bet'cha land understand\",\n",
       " \"Bet n't reprimand daughters\",\n",
       " 'Bright young women sick swimming',\n",
       " 'Ready stand',\n",
       " '',\n",
       " \"I 'm ready know people know\",\n",
       " \"Ask 'em questions get answers\",\n",
       " \"What 's fire\",\n",
       " \"What 's word Burn\",\n",
       " '',\n",
       " \"When 's turn Would n't I love\",\n",
       " 'Love explore shore',\n",
       " 'Out sea wish I could',\n",
       " 'Part world']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_word(corpus):\n",
    "  uniqueWords = set()\n",
    "  for sentence in corpus:\n",
    "    sentence = word_tokenize(sentence)\n",
    "    for word in sentence:\n",
    "      uniqueWords.add(word)\n",
    "  return uniqueWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Would', 'could', 'run', 'stand', 'jumping', 'sand', 'thingamabob', 'explore', 'galore', 'walk', 'many', 'Wanderin', 'feet', 'To', 'pay', 'But', 'wonders', 'stuff', 'collection', 'women', 'free', 'Up', 'girl', 'daughters', 'Street', 'twenty', 'want', 'call', 'one', 'love', 'gadgets', 'wan', 'Love', 'stay', 'around', 'world', 'Oh', 'get', \"Bet'cha\", 'Ready', 'think', 'trove', 'neat', 'deal', 'Sure', \"n't\", 'Want', 'aplenty', 'whozits', 'cares', 'people', 'fire', 'sea', 'Part', 'Walking', 'untold', 'far', 'warm', 'No', 'big', 'day', 'Look', 'What', 'Ask', 'Strolling', 'give', 'I', 'Flippin', 'required', \"'em\", 'Burn', 'would', 'fins', 'whatzits', 'swimming', 'land', 'dancing', 'How', \"'ve\", \"'\", 'see', 'wish', 'turn', 'na', 'complete', 'hold', 'ready', 'cavern', 'If', 'spend', 'shore', 'young', 'waters', 'know', \"'m\", \"'s\", 'understand', 'When', 'along', 'questions', 'sun', 'word', 'sick', 'answers', 'gizmos', 'treasures', 'Legs', 'Looking', 'live', 'Bright', 'Out', 'everything', 'Bet', 'The', 'got', 'reprimand'}\n"
     ]
    }
   ],
   "source": [
    "docss=extract_unique_word(cleanedData)\n",
    "print(docss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MHDAH\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>above</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.41645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>again</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>along</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.298403</td>\n",
       "      <td>0.360041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wouldn</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.388439</td>\n",
       "      <td>0.468496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377852</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344944</td>\n",
       "      <td>0.416036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.368962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1         2         3    4    5    6    7         8    9   ...  \\\n",
       "above   0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  ...   \n",
       "again   0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  ...   \n",
       "all     0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  ...   \n",
       "along   0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  ...   \n",
       "and     0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  ...   \n",
       "...     ...  ...       ...       ...  ...  ...  ...  ...       ...  ...  ...   \n",
       "would   0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  ...   \n",
       "wouldn  0.0  0.0  0.388439  0.468496  0.0  0.0  0.0  0.0  0.000000  0.0  ...   \n",
       "you     0.0  0.0  0.344944  0.416036  0.0  0.0  0.0  0.0  0.368962  0.0  ...   \n",
       "young   0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  ...   \n",
       "your    0.0  0.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  ...   \n",
       "\n",
       "         44   45        46        47   48   49        50       51   52   53  \n",
       "above   0.0  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.41645  0.0  0.0  \n",
       "again   0.0  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.00000  0.0  0.0  \n",
       "all     0.0  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.00000  0.0  0.0  \n",
       "along   0.0  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.00000  0.0  0.0  \n",
       "and     0.0  0.0  0.298403  0.360041  0.0  0.0  0.000000  0.00000  0.0  0.0  \n",
       "...     ...  ...       ...       ...  ...  ...       ...      ...  ...  ...  \n",
       "would   0.0  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.00000  0.0  0.0  \n",
       "wouldn  0.0  0.0  0.000000  0.000000  0.0  0.0  0.377852  0.00000  0.0  0.0  \n",
       "you     0.0  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.00000  0.0  0.0  \n",
       "young   0.0  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.00000  0.0  0.0  \n",
       "your    0.0  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.00000  0.0  0.0  \n",
       "\n",
       "[144 rows x 54 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# It fits the data and transform it as a vector\n",
    "X = vectorizer.fit_transform(docs)\n",
    "# Convert the X as transposed matrix\n",
    "X = X.T.toarray()\n",
    "# Create a DataFrame and set the vocabulary as the index\n",
    "df = pd.DataFrame(X, index=vectorizer.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(corpus):\n",
    "  index = dict()\n",
    "  uniqueWords = extract_unique_word(corpus)\n",
    "  for word in uniqueWords:\n",
    "    temp = []\n",
    "    for i, sentence in enumerate(corpus):\n",
    "      sentence = word_tokenize(sentence)\n",
    "      if word in sentence:\n",
    "        temp.append(i)\n",
    "    index[word] = temp\n",
    "  return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Would': [2, 3, 50], 'could': [32, 36, 52], 'run': [30], 'stand': [43], 'jumping': [25], 'sand': [38], 'thingamabob': [13], 'explore': [51], 'galore': [12], 'walk': [30], 'many': [7], 'Wanderin': [32], 'feet': [22], 'To': [38], 'pay': [37], 'But': [15], 'wonders': [7], 'stuff': [1], 'collection': [2], 'women': [42], 'free': [32], 'Up': [30, 31], 'girl': [3, 4], 'daughters': [41], 'Street': [28], 'twenty': [14], 'want': [16], 'call': [21], 'one': [7], 'love': [50], 'gadgets': [11], 'wan': [18, 19], 'Love': [51], 'stay': [31], 'around': [8, 20], 'world': [33, 53], 'Oh': [22], 'get': [24, 46], \"Bet'cha\": [40], 'Ready': [43], 'think': [2, 3, 8], 'trove': [6], 'neat': [1], 'deal': [15], 'Sure': [9], \"n't\": [1, 2, 3, 24, 41, 50], 'Want': [13], 'aplenty': [11], 'whozits': [12], 'cares': [15], 'people': [18, 45], 'fire': [47], 'sea': [52], 'Part': [33, 53], 'Walking': [20], 'untold': [6], 'far': [24], 'warm': [38], 'No': [15], 'big': [15], 'day': [31, 38], 'Look': [1, 6], 'What': [21, 27, 35, 37, 47, 48], 'Ask': [46], 'Strolling': [26], 'give': [35], 'I': [3, 11, 12, 14, 16, 18, 19, 32, 35, 36, 37, 45, 50, 52], 'Flippin': [24], 'required': [25], \"'em\": [21, 46], 'Burn': [48], 'would': [35, 37], 'fins': [24], 'whatzits': [12], 'swimming': [42], 'land': [40], 'dancing': [19, 25], 'How': [7], \"'ve\": [11, 12, 14], \"'\": [19, 24, 32], 'see': [19], 'wish': [32, 52], 'turn': [50], 'na': [18, 19], 'complete': [2], 'hold': [7], 'ready': [45], 'cavern': [7], 'If': [36], 'spend': [38], 'shore': [51], 'young': [42], 'waters': [36], 'know': [45], \"'m\": [3, 45], \"'s\": [2, 9, 27, 47, 48, 50], 'understand': [40], 'When': [50], 'along': [26], 'questions': [46], 'sun': [31], 'word': [27, 48], 'sick': [42], 'answers': [46], 'gizmos': [11], 'treasures': [6], 'Legs': [25], 'Looking': [8], 'live': [36], 'Bright': [42], 'Out': [52], 'everything': [4, 9], 'Bet': [41], 'The': [4], 'got': [9, 11, 12, 14], 'reprimand': [41]}\n"
     ]
    }
   ],
   "source": [
    "index = build_index(cleanedData)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_serach(query):\n",
    "  queryWords = word_tokenize(query)\n",
    "  result = []\n",
    "  for queryWord in queryWords:\n",
    "    if queryWord in list(index.keys()):\n",
    "      result.append(index[queryWord])\n",
    "  i = 0 \n",
    "  finalResult = []\n",
    "  for j in range(1,len(result)):\n",
    "    finalResult.append(set(result[i]) & set(result[j]))\n",
    "    i = j\n",
    "  return finalResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "and_serach(\"call What\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_serach(query):\n",
    "  queryWords = word_tokenize(query)\n",
    "  result = []\n",
    "  for queryWord in queryWords:\n",
    "    if queryWord in list(index.keys()):\n",
    "      result.append(index[queryWord])\n",
    "  finalResult = set()\n",
    "  for document in result:\n",
    "    for i in document:\n",
    "      finalResult.add(i)\n",
    "  return finalResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_serach(\"call What\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_query(query):\n",
    "  orginal_index = list(range(0,len(index)))\n",
    "  query = query.split()\n",
    "  result = []\n",
    "  expression = ''\n",
    "  for i,word in enumerate(query):\n",
    "    if i == 1:\n",
    "      expression = query[i-1] + \" \" + query[i+1]\n",
    "      #print(expression)\n",
    "      if word == 'and':\n",
    "        result = and_serach(expression)\n",
    "        #print(result)\n",
    "      elif word == 'or':\n",
    "        result = or_serach(expression)\n",
    "    elif i > 1:\n",
    "      if word == 'and':\n",
    "        result = result & index[query[i+1]]\n",
    "      elif word == 'or' :\n",
    "        result = result + index[query[i+1]]\n",
    "      elif word == 'not':\n",
    "        indexes = index[query[i+1]]\n",
    "        not_ = list(set(orginal_index) - set(indexes))\n",
    "        result = result[-1] & set(not_)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_query(\"call and What or untold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('call')#a set of synonyms that share a common meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "\n",
    "wn.path_similarity(dog, cat)# Return a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89ecb3e4d330a29e61af272fcfa506874897f976c0e7747a704629af50a82761"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
